{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjfAQmwpFYTJ"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "#  Setup: install packages\n",
        "# ============================\n",
        "!pip install -q torch transformers peft accelerate bitsandbytes scikit-learn nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# ============================\n",
        "#  Clone your GitHub repo\n",
        "# ============================\n",
        "# TODO: replace with your actual repo URL (HTTPS)\n",
        "REPO_URL = \"https://github.com/YOUR_USERNAME/YOUR_REPO_NAME.git\"\n",
        "\n",
        "!git clone {REPO_URL}\n",
        "\n",
        "# If your repo name is different, change this line accordingly:\n",
        "%cd YOUR_REPO_NAME\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "MODEL_ID = \"Iftakhar/deepseek-phi-adapter\"\n",
        "\n",
        "\n",
        "def get_repo_root() -> Path:\n",
        "    # We are already cd-ed into repo root in Colab\n",
        "    return Path(os.getcwd())\n",
        "\n",
        "\n",
        "def get_data_path() -> Path:\n",
        "    return get_repo_root() / \"data\" / \"phi_data.jsonl\"\n",
        "\n",
        "\n",
        "def load_data(path: Path, max_samples: int | None = None):\n",
        "    data = []\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            j = json.loads(line)\n",
        "            data.append(\n",
        "                {\n",
        "                    \"instruction\": j.get(\"input\", \"\"),\n",
        "                    \"ground_truth\": j.get(\"output\", {}).get(\"redacted_text\", \"\"),\n",
        "                    \"raw\": j,\n",
        "                }\n",
        "            )\n",
        "    if max_samples is not None and len(data) > max_samples:\n",
        "        data = random.sample(data, max_samples)\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_model_and_tokenizer():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    if \"<END>\" not in tokenizer.get_vocab():\n",
        "        tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<END>\"]})\n",
        "    end_id = tokenizer.convert_tokens_to_ids(\"<END>\") if \"<END>\" in tokenizer.get_vocab() else tokenizer.eos_token_id\n",
        "\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        quantization_config=bnb_config,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    model.eval()\n",
        "\n",
        "    return model, tokenizer, device, end_id\n",
        "\n",
        "\n",
        "def generate_redaction(model, tokenizer, device, end_id, instruction: str) -> str:\n",
        "    prompt = f\"### Instruction:\\n{instruction}\\n\\n### Output:\\n\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    gen_kwargs = dict(\n",
        "        max_new_tokens=384,\n",
        "        temperature=0.0,\n",
        "        top_p=0.9,\n",
        "        eos_token_id=end_id,\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, **gen_kwargs)\n",
        "\n",
        "    decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    if \"### Output:\" in decoded:\n",
        "        decoded = decoded.split(\"### Output:\", 1)[1]\n",
        "    if \"<END>\" in decoded:\n",
        "        decoded = decoded.split(\"<END>\", 1)[0]\n",
        "    return decoded.strip()\n",
        "\n",
        "\n",
        "# ---------- RUN DEMO ----------\n",
        "data_path = get_data_path()\n",
        "if not data_path.exists():\n",
        "    raise FileNotFoundError(f\"Expected data file at: {data_path}\")\n",
        "\n",
        "print(f\" Loading data from: {data_path}\")\n",
        "samples = load_data(data_path, max_samples=5)  # change 5 -> any number you like\n",
        "\n",
        "print(f\" Loading model from Hugging Face: {MODEL_ID}\")\n",
        "model, tokenizer, device, end_id = load_model_and_tokenizer()\n",
        "\n",
        "print(\"\\n=================  DEMO OUTPUTS =================\")\n",
        "for idx, sample in enumerate(samples, start=1):\n",
        "    print(\"=\" * 100)\n",
        "    print(f\" SAMPLE {idx}\")\n",
        "    print(\" Instruction:\")\n",
        "    print(sample[\"instruction\"])\n",
        "    print(\"\\n  Generating redacted output...\")\n",
        "    pred = generate_redaction(model, tokenizer, device, end_id, sample[\"instruction\"])\n",
        "\n",
        "    print(\"\\n Model Output:\")\n",
        "    print(pred)\n",
        "    print(\"\\n Ground Truth:\")\n",
        "    print(sample[\"ground_truth\"])\n",
        "    print()\n",
        "\n",
        "print(\"\\nDemo complete.\")\n"
      ],
      "metadata": {
        "id": "yGVvfajNaxTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "MODEL_ID = \"Iftakhar/deepseek-phi-adapter\"\n",
        "\n",
        "\n",
        "def get_repo_root() -> Path:\n",
        "    return Path(os.getcwd())\n",
        "\n",
        "\n",
        "def get_data_path() -> Path:\n",
        "    return get_repo_root() / \"data\" / \"phi_data.jsonl\"\n",
        "\n",
        "\n",
        "def read_phi_jsonl(path: Path):\n",
        "    data = []\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            j = json.loads(line)\n",
        "            data.append(\n",
        "                {\n",
        "                    \"instruction\": j.get(\"input\", \"\"),\n",
        "                    \"ground_truth\": j.get(\"output\", {}).get(\"redacted_text\", \"\"),\n",
        "                    \"split\": j.get(\"split\", None),\n",
        "                    \"raw\": j,\n",
        "                }\n",
        "            )\n",
        "    return data\n",
        "\n",
        "\n",
        "def maybe_filter_test(data):\n",
        "    if any(d.get(\"split\") == \"test\" for d in data):\n",
        "        return [d for d in data if d.get(\"split\") == \"test\"]\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_model_and_tokenizer():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    if \"<END>\" not in tokenizer.get_vocab():\n",
        "        tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<END>\"]})\n",
        "    end_id = tokenizer.convert_tokens_to_ids(\"<END>\") if \"<END>\" in tokenizer.get_vocab() else tokenizer.eos_token_id\n",
        "\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        quantization_config=bnb_config,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    model.eval()\n",
        "\n",
        "    return model, tokenizer, device, end_id\n",
        "\n",
        "\n",
        "GEN_KW = dict(max_new_tokens=384, temperature=0.6, top_p=0.9)\n",
        "safe_div = lambda a, b: a / b if b > 0 else 0.0\n",
        "\n",
        "\n",
        "def clean_output(txt: str) -> str:\n",
        "    txt = txt.split(\"<END>\")[0]\n",
        "    txt = re.sub(r\"(RecordedVote|<HR>|Ã½:).*\", \"\", txt, flags=re.DOTALL)\n",
        "    return txt.strip()\n",
        "\n",
        "\n",
        "def extract_phi_tags(text: str):\n",
        "    return re.findall(r\"<REDACTED:([A-Z_]+)>\", text)\n",
        "\n",
        "\n",
        "def classify_policy(prompt: str) -> str:\n",
        "    p = prompt.lower()\n",
        "    if \"all phi\" in p or \"everything\" in p:\n",
        "        return \"Redact All PHI\"\n",
        "    elif \"do not\" in p or \"keep all\" in p:\n",
        "        return \"Do Not Redact\"\n",
        "    elif \"only\" in p:\n",
        "        if \"hospital\" in p or \"facility\" in p:\n",
        "            return \"Facility Only\"\n",
        "        elif \"vehicle\" in p or \"vin\" in p:\n",
        "            return \"Vehicle Only\"\n",
        "        elif \"date\" in p:\n",
        "            return \"Date Only\"\n",
        "        elif \"identifier\" in p or \"ssn\" in p:\n",
        "            return \"Identifier Only\"\n",
        "        else:\n",
        "            return \"Selective\"\n",
        "    elif \"except\" in p:\n",
        "        return \"Except Some PHI\"\n",
        "    else:\n",
        "        return \"General\"\n",
        "\n",
        "\n",
        "def detect_hallucination(original_text: str, model_output: str) -> bool:\n",
        "    original = original_text.lower()\n",
        "    output = model_output\n",
        "\n",
        "    new_entities = re.findall(\n",
        "        r\"(mrn\\d+|vin\\d+|ssn\\s*\\d+|\\b[A-Z][a-z]+\\s[A-Z][a-z]+)\", output\n",
        "    )\n",
        "    hallucinated = [e for e in new_entities if e.lower() not in original]\n",
        "    return len(hallucinated) > 0\n",
        "\n",
        "\n",
        "def generate_output(model, tokenizer, device, end_id, instruction: str) -> str:\n",
        "    prompt = f\"### Instruction:\\n{instruction}\\n\\n### Output:\\n\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        gen = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=GEN_KW[\"max_new_tokens\"],\n",
        "            temperature=GEN_KW[\"temperature\"],\n",
        "            top_p=GEN_KW[\"top_p\"],\n",
        "            eos_token_id=end_id,\n",
        "        )\n",
        "    out = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
        "    if \"### Output:\" in out:\n",
        "        out = out.split(\"### Output:\", 1)[1]\n",
        "    model_out = clean_output(out)\n",
        "    return model_out\n",
        "\n",
        "\n",
        "# ------------------- RUN EVALUATION -------------------\n",
        "data_path = get_data_path()\n",
        "if not data_path.exists():\n",
        "    raise FileNotFoundError(f\"Expected data file at: {data_path}\")\n",
        "\n",
        "print(f\" Loading data from: {data_path}\")\n",
        "all_data = read_phi_jsonl(data_path)\n",
        "test_data = maybe_filter_test(all_data)\n",
        "\n",
        "if len(test_data) == 0:\n",
        "    raise ValueError(\"No test data found in phi_data.jsonl.\")\n",
        "\n",
        "NUM_SAMPLES = 100  # change this if you want more/less\n",
        "num_samples = min(NUM_SAMPLES, len(test_data))\n",
        "samples = random.sample(test_data, num_samples)\n",
        "print(f\" Evaluating on {num_samples} samples.\")\n",
        "\n",
        "print(f\"Loading model from Hugging Face: {MODEL_ID}\")\n",
        "model, tokenizer, device, end_id = load_model_and_tokenizer()\n",
        "\n",
        "metrics_by_policy = defaultdict(\n",
        "    lambda: {\"TP\": 0, \"FP\": 0, \"FN\": 0, \"BLEU\": [], \"Trust\": [], \"Hall\": 0, \"Count\": 0}\n",
        ")\n",
        "preds, refs = [], []\n",
        "\n",
        "# MAIN LOOP\n",
        "for sample in samples:\n",
        "    gt = sample[\"ground_truth\"]\n",
        "    instr = sample[\"instruction\"]\n",
        "\n",
        "    model_out = generate_output(model, tokenizer, device, end_id, instr)\n",
        "\n",
        "    pred_tags = extract_phi_tags(model_out)\n",
        "    gt_tags = extract_phi_tags(gt)\n",
        "\n",
        "    policy = classify_policy(instr)\n",
        "    m = metrics_by_policy[policy]\n",
        "    m[\"Count\"] += 1\n",
        "\n",
        "    pred_set, gt_set = set(pred_tags), set(gt_tags)\n",
        "    TP = len(pred_set & gt_set)\n",
        "    FP = len(pred_set - gt_set)\n",
        "    FN = len(gt_set - pred_set)\n",
        "\n",
        "    m[\"TP\"] += TP\n",
        "    m[\"FP\"] += FP\n",
        "    m[\"FN\"] += FN\n",
        "\n",
        "    is_hall = detect_hallucination(instr, model_out)\n",
        "    m[\"Hall\"] += 1 if is_hall else 0\n",
        "\n",
        "    try:\n",
        "        bleu = sentence_bleu([gt.split()], model_out.split())\n",
        "    except Exception:\n",
        "        bleu = 0.0\n",
        "    m[\"BLEU\"].append(bleu)\n",
        "    trust = (1 - int(is_hall)) * bleu\n",
        "    m[\"Trust\"].append(trust)\n",
        "\n",
        "    preds.append(model_out)\n",
        "    refs.append(gt)\n",
        "\n",
        "# AGGREGATE\n",
        "print(\"\\n=================  EVALUATION SUMMARY =================\")\n",
        "macro_F1s = []\n",
        "total_TP = total_FP = total_FN = 0\n",
        "total_hall = total_count = 0\n",
        "\n",
        "for pol, m in metrics_by_policy.items():\n",
        "    total_TP += m[\"TP\"]\n",
        "    total_FP += m[\"FP\"]\n",
        "    total_FN += m[\"FN\"]\n",
        "    total_hall += m[\"Hall\"]\n",
        "    total_count += m[\"Count\"]\n",
        "\n",
        "    P = safe_div(m[\"TP\"], m[\"TP\"] + m[\"FP\"])\n",
        "    R = safe_div(m[\"TP\"], m[\"TP\"] + m[\"FN\"])\n",
        "    F1 = safe_div(2 * P * R, P + R)\n",
        "    HallRate = safe_div(m[\"Hall\"], m[\"Count\"])\n",
        "    BLEU = sum(m[\"BLEU\"]) / max(1, len(m[\"BLEU\"]))\n",
        "    Trust = sum(m[\"Trust\"]) / max(1, len(m[\"Trust\"]))\n",
        "    macro_F1s.append(F1)\n",
        "\n",
        "    print(f\"\\n Policy: {pol}\")\n",
        "    print(f\"Samples: {m['Count']}\")\n",
        "    print(f\"Precision: {P:.3f}, Recall: {R:.3f}, F1: {F1:.3f}\")\n",
        "    print(f\"BLEU: {BLEU:.3f}, Hallucination Rate: {HallRate*100:.1f}%\")\n",
        "    print(f\"Trust Score: {Trust:.3f}\")\n",
        "\n",
        "overall_microP = safe_div(total_TP, total_TP + total_FP)\n",
        "overall_microR = safe_div(total_TP, total_TP + total_FN)\n",
        "overall_microF1 = safe_div(2 * overall_microP * overall_microR, overall_microP + overall_microR)\n",
        "macroF1 = sum(macro_F1s) / len(macro_F1s) if macro_F1s else 0.0\n",
        "overall_HallRate = safe_div(total_hall, total_count)\n",
        "\n",
        "correct_policies = sum(1 for s in samples if classify_policy(s[\"instruction\"]) != \"General\")\n",
        "PMA = safe_div(correct_policies, len(samples))\n",
        "\n",
        "# char-level F1 with alignment\n",
        "chars_true, chars_pred = [], []\n",
        "for gt, pr in zip(refs, preds):\n",
        "    L = min(len(gt), len(pr))\n",
        "    if L == 0:\n",
        "        continue\n",
        "    chars_true.extend(list(gt[:L]))\n",
        "    chars_pred.extend(list(pr[:L]))\n",
        "\n",
        "if chars_true and chars_pred:\n",
        "    P_c, R_c, F_c, _ = precision_recall_fscore_support(\n",
        "        chars_true,\n",
        "        chars_pred,\n",
        "        average=\"micro\",\n",
        "    )\n",
        "else:\n",
        "    P_c = R_c = F_c = 0.0\n",
        "\n",
        "print(\"\\n------------------------------------------------------------\")\n",
        "print(f\" Overall Micro Precision: {overall_microP:.3f}\")\n",
        "print(f\"Overall Micro Recall:    {overall_microR:.3f}\")\n",
        "print(f\" Overall Micro F1:        {overall_microF1:.3f}\")\n",
        "print(f\" Macro F1 (avg across policies): {macroF1:.3f}\")\n",
        "print(f\" Hallucination Rate (overall): {overall_HallRate*100:.1f}%\")\n",
        "print(f\" Policy Match Accuracy (PMA): {PMA:.3f}\")\n",
        "print(f\"Char-level F1: {F_c:.3f} (Precision {P_c:.3f}, Recall {R_c:.3f})\")\n",
        "\n",
        "print(\"\\n================= ðŸ§¾ QUALITATIVE EXAMPLES =================\")\n",
        "for i, (s, pred) in enumerate(zip(samples[:5], preds[:5]), 1):\n",
        "    print(\"=\" * 120)\n",
        "    print(f\" SAMPLE {i} | Policy: {classify_policy(s['instruction'])}\")\n",
        "    print(\" Instruction:\", s[\"instruction\"])\n",
        "    print(\"\\n Model Output:\\n\", pred)\n",
        "    print(\"\\n Ground Truth:\\n\", s[\"ground_truth\"])\n",
        "\n",
        "print(\"\\n Evaluation complete â€” all metrics computed successfully.\")\n"
      ],
      "metadata": {
        "id": "d3XTCHPDbJqu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}