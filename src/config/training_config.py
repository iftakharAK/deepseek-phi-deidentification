# -*- coding: utf-8 -*-
"""training_config.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sM-2fSk4ch-UxD0OE8mefD-hzN0Ded8l
"""

# src/config/training_config.py

from dataclasses import dataclass, field
import os
import time
from typing import Tuple


@dataclass
class TrainingConfig:
    # Model / data
    base_model_dir: str = field(
        default_factory=lambda: os.environ.get(
            "BASE_MODEL_DIR",
            "deepseek-ai/deepseek-llm-7b-base",  # or your local path
        )
    )
    max_seq_len: int = 1536

    # LoRA
    lora_r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.05
    target_modules: Tuple[str, ...] = ("q_proj", "k_proj", "v_proj", "o_proj")

    # Training hyperparameters
    epochs: int = 3
    batch_train: int = 1
    batch_eval: int = 1
    grad_accum: int = 16
    learning_rate: float = 1e-4
    warmup_ratio: float = 0.1
    save_steps: int = 200
    log_steps: int = 20

    # Dataset splitting
    train_ratio: float = 0.8
    val_ratio: float = 0.1
    seed: int = 42

    # Run ID
    run_id: str = field(default_factory=lambda: time.strftime("%Y%m%d-%H%M%S"))